{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7e6e15",
   "metadata": {},
   "source": [
    "I get the list of NASDAQ-traded stocks from CRSP tool (permno.txt). I then input that into CRSP to get the corresponding price data (no fundamentals or sectoral info available from CRSP). That is crsp_data; I then exclude the stocks never traded on the NASDAQ to generate checked_permno.txt. I then input this into CRSP-Compustat merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f06dcb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f672a813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.rename(columns={\"datadate\": \"Date\",\n",
    "                            \"tic\":\"Ticker\",\"conm\":\"Company\", \"datacqtr\":\"Calendar Quarter\",\"datafqtr\":\"Fiscal Quarter\", \n",
    "                            \"dpq\":\"Depreciation and Amortisation\",\n",
    "                            \"wcapq\":\"Working Capital\",\n",
    "                            \"ggroup\":\"Group\",\"gind\":\"Industry\",\"gsector\":\"Sector\",\"gsubind\":\"Subindustry\",\n",
    "                            \"cshopq\":\"Total Shares Repurchased\", \"cshoq\":\"Common Shares Outstanding\",\n",
    "                            \"dlcq\":\"Debt in Current Liabilities\",\"dlttq\":\"Total Long Term Debt\",\n",
    "                            \"niq\":\"Net Income\",\"dltisy\":\"Long Term Debt Issuance YTD\",\n",
    "                            \"dltry\":\"Long Term Debt Reduction YTD\",\n",
    "                            \"aqcy\":\"Acquisitions YTD\",\"capxy\": \"Capital Expenditures YTD\",\n",
    "                            \"dvpq\":\"Preferred Dividends\", \"saleq\":\"Sales\", \"ceqq\":\"Book Value\",#i.e. common equity\n",
    "                            \"xrdq\":\"R&D Expenses\",\n",
    "                            \"cheq\":\"Cash and Short Term Investments\",\n",
    "                            \"cshtrq\":\"Common Shares Traded\",\n",
    "                            \"dvpsxq\":\"Dividends\",\"prccq\":\"Price\"}) # dividends per share ex-date\n",
    "                            # for rename, extra labels listed donâ€™t throw an error by default.\n",
    "    \n",
    "    df = df[df[\"Price\"].notna()]\n",
    "    df[\"Date\"]=pd.to_datetime(df[\"Date\"], format='%Y%m%d')\n",
    "    \n",
    "    df= df.sort_values(by=['LPERMNO','Date','fqtr'])\n",
    "    \n",
    "    df = df[(df[\"Net Income\"].notna())|(df[\"Dividends\"]!=0)]\n",
    "    df = df.groupby([\"LPERMNO\"]).filter(lambda x: len(x) > 8) # minimum length to run bubble tests should be 9 \n",
    "    df = df.drop_duplicates(subset = [\"LPERMNO\",\"Calendar Quarter\"], keep =\"last\")\n",
    "    df = df.drop_duplicates(subset = [\"LPERMNO\",\"Date\"], keep =\"last\").reset_index(drop = True)\n",
    "    # this is robust- I sorted by fiscal quarter (i.e. 1,2,3,4) above and it is indeed best to keep the last value.\n",
    "    # These duplicates seem to be the same in every way except for the fiscal quarter\n",
    "    # and the row with the lastest fiscal quarter seems to be the most complete.\n",
    "    \n",
    "    df = df.drop(columns = [\"indfmt\",\"consol\",\"popsrc\",\"datafmt\",\"exchg\",\"curcdq\",\"costat\",\"fyearq\",\"mkvaltq\",\"invchy\",\"actq\",\"lctq\"],errors='ignore')\n",
    "     #drop market value because it's mostly missing. Just use shares outstanding instead.\n",
    "    # invchy after adjusted for ytd is just the decrease in total inventories invtq, same thing.\n",
    "                        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75c8b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(r\"compustat data v4.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced3c577",
   "metadata": {},
   "source": [
    "Sorting out sectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc44aff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for permno, group in df.groupby(\"LPERMNO\"):\n",
    "    df.loc[df.LPERMNO==permno,\"Sector\"]= group[\"Sector\"].iloc[-1]\n",
    "    # on rare occasions a company changes sector.\n",
    "    # there's only 30 cases like this or something so I just make each company's sector the last one it was in.\n",
    "    \n",
    "for permno, group in df.groupby(\"LPERMNO\"):\n",
    "    if len(group[\"Sector\"].unique())>1:\n",
    "        print(permno)\n",
    "        \n",
    "for i in [\"Sector\",\"Group\",\"Industry\",\"Subindustry\"]:\n",
    "    df[i].fillna(56,inplace=True)\n",
    "    df[i] = df[i].astype(int).astype(str)\n",
    "    df[i] = df[i].replace(\"56\",\"Other\")\n",
    "df[\"Sector\"].replace({\"45\":\"Information Technology\",\"50\":\"Communication Services\",\"20\":\"Industrials\",\n",
    "              \"25\":\"Consumer Discretionary\",\"35\":\"Healthcare\",\"30\":\"Consumer Staples\",\"15\":\"Materials\",\n",
    "              \"55\":\"Utilities\",\"10\":\"Energy\",\"60\":\"Real Estate\",\"40\":\"Financials\",\"Other\":\"Other\"},inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9967ae",
   "metadata": {},
   "source": [
    "Cross-check stock exchange data between CRSP and CCM. I also checked: using compustat bank fundamentals gives the same underlying statistics. Now, banks do have other factors but that is beyond the scope of this study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96741d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11544 / 11544\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv(r\"crsp data.csv\")\n",
    "df2 = df2.rename(columns={\"date\": \"Date\", \"EXCHCD\": \"Exchange Code\",\n",
    "                          \"TICKER\":\"Ticker\",\"COMNAM\":\"Company\",\n",
    "                          \"DIVAMT\":\"Dividend\",\"PRC\":\"Price\",\"VOL\":\"Volume\",\"SHROUT\":\"Shares Outstanding\"\n",
    "                         })\n",
    "df2 = df2[df2[\"Price\"].notna()]\n",
    "df2 = df2[df2[\"Exchange Code\"]==3]\n",
    "df2[\"Dividend\"] = df2[\"Dividend\"].fillna(0)\n",
    "# CRSP's hyphenated price values means there was no close price, but instead a bid/ask average.\n",
    "df2[\"Price\"]= df2[\"Price\"].abs()\n",
    "\n",
    "df2[\"Date\"]=pd.to_datetime(df2[\"Date\"], format='%Y%m%d', errors='ignore')\n",
    "\n",
    "df2= df2.sort_values(by=['PERMNO','Date','Dividend']) \n",
    "# things are mostly ordered, but I want to sort by dividend too in order to drop duplicates and keep the largest dividend entry.\n",
    "\n",
    "df2 = df2.drop_duplicates(subset = [\"PERMNO\",\"Date\"], keep =\"last\").reset_index(drop = True)\n",
    "\n",
    "j = 0\n",
    "length = len(df[\"LPERMNO\"].unique())\n",
    "\n",
    "df[\"Exchange Check\"] = 0\n",
    "\n",
    "for permno, group in df.groupby(\"LPERMNO\"):\n",
    "    clear_output(wait=True)\n",
    "    match = df2[df2[\"PERMNO\"]==permno]\n",
    "    firstdate = match[\"Date\"].iloc[0]\n",
    "    lastdate = match[\"Date\"].iloc[-1]\n",
    "    # aim here is to remove all observations that weren't when the stock was listed on the NASDAQ\n",
    "    df.loc[(df.LPERMNO ==permno) & (df.Date < firstdate),\"Exchange Check\"] = 1\n",
    "    df.loc[(df.LPERMNO ==permno) & (df.Date > lastdate),\"Exchange Check\"]=1 # these are the observations I want to drop!\n",
    "    # this is much quicker than dropping observations in place\n",
    "    j+=1\n",
    "    print(j,\"/\",length)\n",
    "    \n",
    "df = df[df[\"Exchange Check\"]==0].reset_index(drop = True)\n",
    "    \n",
    "df.drop(\"Exchange Check\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae9b8f0",
   "metadata": {},
   "source": [
    "Date cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb31c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some specific errors I found\n",
    "df.loc[(df.LPERMNO ==77862)&(df.Date ==\"1993-04-30\"),\"Calendar Quarter\"] = \"1993Q1\"\n",
    "df.loc[df.LPERMNO ==11453,\"Calendar Quarter\"] = df[\"Fiscal Quarter\"]\n",
    "df.loc[(df.LPERMNO ==11665)&(df.Date==\"1993-04-30\"),\"Calendar Quarter\"] = \"1993Q1\"\n",
    "df.loc[(df.LPERMNO ==11665)&(df.Date==\"1993-04-30\"),\"fqtr\"] = 1\n",
    "df.loc[(df.LPERMNO ==60281)&(df.Date==\"1975-04-30\"),\"Calendar Quarter\"] = \"1975Q1\"\n",
    "df.loc[(df.LPERMNO ==77040)&(df.Date==\"1996-07-31\"),\"Calendar Quarter\"] = \"1996Q3\"\n",
    "df.loc[(df.LPERMNO ==77040)&(df.Date==\"1996-10-31\"),\"Calendar Quarter\"] = \"1996Q4\"\n",
    "df.drop(df[(df.LPERMNO ==79672)&(df.Date==\"1994-04-30\")].index,inplace=True,errors=\"ignore\")\n",
    "df.loc[(df.LPERMNO ==80063)&(df.Date==\"1995-04-30\"),\"Calendar Quarter\"] = \"1995Q1\"\n",
    "\n",
    "df[\"Dividends\"] = df[\"Dividends\"].fillna(0)\n",
    "df[\"Preferred Dividends\"] = df[\"Preferred Dividends\"].fillna(0)\n",
    "\n",
    "# standardising quarterly dates\n",
    "df[\"Adjusted Date\"] = pd.to_datetime(df[\"Calendar Quarter\"])+pd.tseries.offsets.QuarterEnd(0)\n",
    "# to keep the format in line with the original data, I want to have the dates at the end of each quarter.\n",
    "# by default, pandas quarter conversion gives the start of the quarter.\n",
    "# if it were the end of quarter, then offsetting that by 1 would actually put me in the next quarter which would be wrong.\n",
    "# here, both 0 and 1 offsets work.\n",
    "\n",
    "df.loc[df[\"Calendar Quarter\"].isna(),\"Adjusted Date\"]= df[\"Date\"] + pd.tseries.offsets.QuarterEnd(0)\n",
    "df = df.drop_duplicates(subset = [\"LPERMNO\",\"Adjusted Date\"], keep =\"last\")\n",
    "\n",
    "# Sometimes a firm may also change accounting standards/its fiscal calendar \n",
    "# it could have a report in both January and April which makes no sense.\n",
    "df[\"Date\"]= df[\"Adjusted Date\"]\n",
    "df.drop(columns=[\"Adjusted Date\"],inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce96b2fd",
   "metadata": {},
   "source": [
    "Script that finds breaks in series and separates them if segments are long enough, or discards if too short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4767cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11519\n"
     ]
    }
   ],
   "source": [
    "#removing gaps\n",
    "df[\"Adjusted_LPERMNO\"]=df[\"LPERMNO\"].astype(str)\n",
    "\n",
    "counter = 0\n",
    "for permno, group in df.groupby(\"LPERMNO\"):\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    indices = list(group.index)\n",
    "    dates =list(group[\"Date\"])\n",
    "    expected_dates = list(pd.date_range(group[\"Date\"].min(),group[\"Date\"].max(),freq=\"Q\"))\n",
    "    # don't need to worry about offsetting to the end of the quarter here\n",
    "    # because the start and end points are already offset.\n",
    "    \n",
    "    if len(dates)!= len(expected_dates):\n",
    "        \n",
    "        start = 0\n",
    "        marker = 0\n",
    "        for i in range(len(dates)-1):\n",
    "            \n",
    "            \n",
    "            if (dates[i+1]-dates[i]).days>92: \n",
    "                # max 92 days in a quarter\n",
    "                # if this condition is met\n",
    "                # then between indices i+1 and i there is a break.'\n",
    "                \n",
    "                if i- start>7:\n",
    "                    \n",
    "                    # so the current segment goes from index 'start' to index i.\n",
    "                    # minimum segment length is 9, so the difference >=8 i.e. >7.\n",
    "                    df.loc[indices[start:i+1],\"Adjusted_LPERMNO\"]+= str(marker)\n",
    "                    marker+=1\n",
    "                    # every time we get a new usable series segment, marker increments.\n",
    "                    \n",
    "                    # remember, everything is already sorted\n",
    "                    # e.g. 10001 is split into 100010, 100011\n",
    "                \n",
    "                else:\n",
    "                    # segment not long enough, so drop\n",
    "                    df.drop(indices[start:i+1], inplace=True)\n",
    "                    # both loc and drop take index arguments as opposed to iloc which takes positional arguments.\n",
    "                    \n",
    "                start = i+1\n",
    "                # increment 'start' (as an index of 'dates') for the next segment\n",
    "                \n",
    "    counter+=1\n",
    "    print(counter)\n",
    "    \n",
    "# If there is a break between the penultimate and final values, the script doesn't do anything to the final value.\n",
    "#Hence I apply this:\n",
    "df = df.groupby([\"Adjusted_LPERMNO\"]).filter(lambda x: len(x) > 8)\n",
    "df = df.reset_index(drop = True)\n",
    "# df[\"LPERMNO\"]= df[\"Adjusted_LPERMNO\"]\n",
    "# df.drop(columns=[\"Adjusted_LPERMNO\"],inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdca053",
   "metadata": {},
   "source": [
    "Unrelated stuff used to generate figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "65815059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df4 = pd.read_excel(r\"us internet.xlsx\")\n",
    "# df4[\"Date\"]=pd.to_datetime(df4[\"Date\"], format='%Y')\n",
    "# fig = px.line(df4,x=\"Date\",y=\"Usage\")\n",
    "# fig.update_layout(\n",
    "#     title=\"US Internet Usage\",\n",
    "#     xaxis_title=\"Date\",\n",
    "#     yaxis_title=\"% of Population\"\n",
    "# )\n",
    "\n",
    "# df4 = pd.read_csv(r\"nasdaq composite.csv\")\n",
    "# df4[\"Date\"]=pd.to_datetime(df4[\"Date\"], format='%Y%m%d')\n",
    "# fig = px.line(df4,x=\"Date\",y=\"Price\")\n",
    "# fig.update_layout(\n",
    "#     title=\"Nasdaq Composite\",\n",
    "#     xaxis_title=\"Date\",\n",
    "#     yaxis_title=\"Points\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361acec5",
   "metadata": {},
   "source": [
    "Now that I've removed gaps, we can transform ytd variables to quarterly.\n",
    "Lots of gaps in FCFE variables...we can run the gap removing algorithm again later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fbbd572",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytd = [\"Long Term Debt Issuance YTD\",\"Long Term Debt Reduction YTD\", \"Acquisitions YTD\",\"Capital Expenditures YTD\"]\n",
    "for i in ytd:\n",
    "    df[i[:-4]]= df.groupby(\"LPERMNO\")[i].diff() \n",
    "    # i.e.\n",
    "    # df[\"Capital Expenditures\"] = df.groupby(\"LPERMNO\")[\"Capital Expenditures YTD\"].diff()\n",
    "    # i[:-4] denotes the column name\n",
    "\n",
    "    df.loc[df.fqtr==1, i[:-4] ] = df[i]\n",
    "    # it resets with each fiscal year\n",
    "    # i.e. capex is same as capex ytd in first quarter, but then for q2 it's the sum of q1 and q2 capex.\n",
    "    df.drop(i,axis=1,inplace=True)\n",
    "    \n",
    "df.loc[df[\"Capital Expenditures\"]<0,\"Capital Expenditures\"] = 0\n",
    "# following https://faculty.wharton.upenn.edu/wp-content/uploads/2017/09/Sikes_20170926.pdf\n",
    "# who also gives ytd explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be0bff5",
   "metadata": {},
   "source": [
    "Calculating the components for FCFE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "576a701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Change in Current Debt\"] = df.groupby(\"LPERMNO\")[\"Debt in Current Liabilities\"].diff()\n",
    "\n",
    "# 1 Definition\n",
    "df[\"Net Borrowing\"] = df[\"Long Term Debt Issuance\"]-df[\"Long Term Debt Reduction\"] +df[\"Change in Current Debt\"]\n",
    "# i.e. increase in long-term debt + increase in current debt\n",
    "# https://dro.dur.ac.uk/18174/1/18174.pdf\n",
    "\n",
    "\n",
    "# 2 Definition\n",
    "\n",
    "df[\"Change in Long Term Debt v2\"] = df.groupby(\"LPERMNO\")[\"Total Long Term Debt\"].diff()\n",
    "df[\"Net Borrowing v2\"] = df[\"Change in Long Term Debt v2\"]+df[\"Change in Current Debt\"]\n",
    "\n",
    "# https://www.cambridge.org/core/journals/journal-of-financial-and-quantitative-analysis/article/where-did-all-the-dollars-go-the-effect-of-cash-flows-on-capital-and-asset-structure/EE06084C564500A2996EF210D26BE54B\n",
    "# these guys use this definition\n",
    "\n",
    "# they're the same in 22% of observations....but most of the time they're not!! I don't know why.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3169760",
   "metadata": {},
   "source": [
    "Non-Cash Working Capital calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1204c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[\"NCWC\"] = df[\"Working Capital\"]-df[\"Cash and Short Term Investments\"]+df[\"Debt in Current Liabilities\"]\n",
    "\n",
    "# define non-cash (net) working capital as \n",
    "# the difference between non-cash current assets and non-debt current liabilities (Damodaran 1999)\n",
    "# so (current assets - cash) - (current liabilities - debt)\n",
    "\n",
    "df[\"Change in NCWC\"] = df.groupby(\"LPERMNO\")[\"NCWC\"].diff()\n",
    "\n",
    "# Can also use accounts receivable, inventories, other assets, payables, taxes payable etc.:\n",
    "# https://www.sciencedirect.com/science/article/pii/S0304405X99000422\n",
    "# but I'm not using this definition anymore\n",
    "\n",
    "df[\"Acquisitions\"] = df[\"Acquisitions\"].fillna(0)\n",
    "# First, any capital expenditures, defined broadly to include acquisitions, are subtracted from the net income... (Damodaran c14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b6153c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 using net borrowing version 1\n",
    "df[\"FCFE\"] = df[\"Net Income\"] - df[\"Capital Expenditures\"] -df[\"Acquisitions\"]+ df[\"Depreciation and Amortisation\"]- df[\"Change in NCWC\"]+df[\"Net Borrowing\"]-df[\"Preferred Dividends\"]\n",
    "#2 using net borrowing version 2\n",
    "df[\"FCFE v2\"] = df[\"Net Income\"] - df[\"Capital Expenditures\"] -df[\"Acquisitions\"]+ df[\"Depreciation and Amortisation\"]- df[\"Change in NCWC\"]+df[\"Net Borrowing v2\"]-df[\"Preferred Dividends\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a47e494",
   "metadata": {},
   "source": [
    "Generating dissertation graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62ed0d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# totals = []\n",
    "# nodivs = []\n",
    "# for sector in df[\"Sector\"].unique():\n",
    "#     total = 0\n",
    "#     nodiv = 0\n",
    "#     df2 = df[df.Sector ==sector]\n",
    "#     for permno, group in df2.groupby(\"LPERMNO\"):\n",
    "#         total+=1\n",
    "#         if (group[\"Dividends\"]==0).all():\n",
    "#             nodiv+=1\n",
    "#     totals.append(total)\n",
    "#     nodivs.append(nodiv)\n",
    "# df2 = pd.DataFrame({\"Sector\":df[\"Sector\"].unique(),\"Total number of Stocks\":totals,\"Non-dividend paying stocks\":nodivs})\n",
    "# df3 = df.groupby(\"Sector\")[[\"Net Income\"]].describe()\n",
    "# merged = pd.merge(df2,df3, left_on=\"Sector\",right_index=True)\n",
    "# merged\n",
    "# merged.to_excel(\"no_dividends_netincome_chart.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "1e62ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df.groupby(\"Date\",as_index=False).count()\n",
    "# fig = px.line(df2,x=\"Date\",y=\"Price\")\n",
    "# fig.update_layout(\n",
    "#     title=\"Number of stocks listed on NASDAQ\",\n",
    "#     xaxis_title=\"Date\",\n",
    "#     yaxis_title=\"Number of stocks\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d7356e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To the degree that cash cannot be invested to earn market returns, and is needed for day-to-day operations,\n",
    "# it is appropriate to look at changes in net working capital, with cash included.\n",
    "# https://pages.stern.nyu.edu/~adamodar/New_Home_Page/CFTheory/deriv/ch14der.html\n",
    "# https://www.wallstreetoasis.com/forums/change-in-net-working-capital-formula\n",
    "# https://quant.stackexchange.com/questions/37477/why-subtract-increase-in-net-working-capital-to-get-free-cash-flows\n",
    "\n",
    "# https://sites.bu.edu/qm222projectcourse/files/2014/08/compustat_users_guide-2003.pdf\n",
    "# market to book if I need it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f5b324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"compustat cleaned v2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "83fa7435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['fqtr',\n",
    "       'Current Assets', 'Cash and Short Term Investments',\n",
    "       'Debt in Current Liabilities',\n",
    "       'Total Long Term Debt', 'Depreciation and Amortisation',\n",
    "       'Preferred Dividends','Current Liabilities','Working Capital', 'Long Term Debt Issuance',\n",
    "       'Long Term Debt Reduction', 'Change in Current Debt',\n",
    "       'Capital Expenditures', 'Net Borrowing',\n",
    "       'Change in Long Term Debt v2', 'Net Borrowing v2', 'NCWC',\n",
    "       'Change in NCWC'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "76233c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"compustat cleaned.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
